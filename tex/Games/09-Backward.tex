\chapter*{Optional: Backward Induction and Its Discontents}

\section*{Subgame Perfect Equilibrium}

Consider the following little game.\footnote{We will not be covering this material in class, or on any assignment, but it is included for people who are interested in further research.} The two players, call them Player I and Player II, have a choice between two options, call them Good and Bad. The game is a sequential move game; first Player I moves then Player II moves. Each player gets 1 if they choose Good and 0 if they choose Bad. We will refer to this game as \gamelab{TheEasyGame}. Here is its game tree.

\begin{figure}[ht]
\begin{center}
\begin{picture}(350, 110)
\pictext{175}{0}{I}
\put(175, 12){\circle*{4}}
\put(175, 12){\line(-2, 1){70}}
\put(175, 12){\line(2, 1){70}}
\pictext{135}{20}{G}
\pictext{215}{20}{B}

\pictext{105}{35}{II}
\put(105, 47){\circle*{4}}
\put(105, 47){\line(-1, 1){35}}
\pictext{70}{85}{(1, 1)}
\put(105, 47){\line(1, 1){35}}
\pictext{140}{85}{(1, 0)}
\pictext{80}{55}{G}
\pictext{130}{55}{B}

\pictext{245}{35}{II}
\put(245, 47){\circle*{4}}
\put(245, 47){\line(-1, 1){35}}
\pictext{210}{85}{(0, 1)}
\put(245, 47){\line(1, 1){35}}
\pictext{280}{85}{(0, 0)}
\pictext{220}{55}{G}
\pictext{270}{55}{B}

%\multiput(105,47)(5, 0){28}{\line(1, 0){3}}

\end{picture}
\end{center}
\caption{\citegame{TheEasyGame}}
\label{PDEForm}
\end{figure}

\noindent A strategy for Player I in \citegame{TheEasyGame} is just a choice of one option, Good or Bad. A strategy for Player II is a little more complicated. She has to choose both what to do if Player I chooses Good, and what to do if Player II chooses Bad. We'll write her strategy as $\alpha \beta$, where $\alpha$ is what she does if Player I chooses Good, and $\beta$ is what she does if Player II chooses Bad. (We will often use this kind of notation in what follows. Wherever it is potentially ambiguous, I'll try to explain it. But the notation is very common in works on game theory, and it is worth knowing.)

The most obvious Nash equilibrium of the game is that Player I chooses Good, and Player II chooses Good whatever Player I does. But there is another Nash \eqm, as looking at the strategic form of the game reveals. We'll put Player I on the row and Player II on the column. (We'll also do this from now on unless there is a reason to do otherwise.)

\starttab{r c c c c}
\citegame{TheEasyGame} & gg & gb & bg & bb \\
G & 1, 1 & \textbf{1, 1} & 1, 0 & 1, 0 \\
B & 0, 1 & 0, 0 & 0, 1 & 0, 0 \\
\fintab Look at the cell that I've bolded, where Player I plays Good, and Player II plays Good, Bad. That's a Nash \eqm. Neither player can improve their outcome, given what the other player plays. But it is a very odd Nash \eqm. It would be very odd for Player II to play this, since it risks getting 0 when they can guarantee getting 1.

It's true that Good, Bad is weakly dominated by Good, Good. But as we've already seen, and as we'll see in very similar examples to this soon, there are dangers in throwing out \textit{all} weakly dominated strategies. Many people think that there is something else wrong with what Player II does here.

Consider the sub-game that starts with the right-hand decision node for Player II. That isn't a very interesting game; Player I has no choices, and Player II simply has a choice between 1 and 0. But it is a game. And note that Good, Bad is not a Nash \eqm\ of that game. Indeed, it is a \textit{strictly} dominated strategy in that game, since it involves taking 0 when 1 is freely available.

Say that a strategy pair is a \textbf{subgame perfect \eqm} when it is a Nash \eqm, and it is a Nash \eqm\ of every sub-game of the game. The pair Good and Good, Bad is not subgame perfect, since it is not a Nash \eqm\ of the right-hand subgame.

When we solve extensive form games by backwards induction, we not only find Nash equilibria, but subgame perfect equilibria. Solving this game by backwards induction would reveal that Player II would choose Good wherever she ends up, and then Player I will play Good at the first move. And the only subgame perfect \eqm\ of the game is that Player I plays Good, and Player II plays Good, Good.

\section*{Problems with Backwards Induction}

When each player only moves once, it is very plausible that each player should play their part of a subgame perfect \eqm\ solution to the game. But this is less compelling when players have multiple moves. We can start to see why by looking at a game that Robert Stalnaker has used in a few places. Again, it is an extensive form game. We will call it \gamelab{Stalnaker}.

\begin{figure}[ht]
\begin{center}
\begin{picture}(260, 100)
\put(20, 80){\line(1,0){220}}

\put(20, 80){\circle*{4}}
\put(20, 80){\line(0, -1){60}}
\pictext{20}{85}{I}
\pictext{60}{65}{$A_1$}
\pictext{28}{45}{$D_1$}
\pictext{20}{8}{2}

\put(100, 80){\circle*{4}}
\put(100, 80){\line(0, -1){60}}
\pictext{100}{85}{II}
\pictext{140}{65}{$a$}
\pictext{108}{45}{$d$}
\pictext{100}{8}{1}

\put(180, 80){\circle*{4}}
\put(180, 80){\line(0, -1){60}}
\pictext{180}{85}{I}
\pictext{220}{65}{$A_2$}
\pictext{188}{45}{$D_2$}
\pictext{180}{8}{0}

\pictext{248}{75}{3}

\end{picture}
\end{center}
\caption{\citegame{Stalnaker}}
\label{PDEForm}
\end{figure}

The game starts in the upper-left corner. There are up to three moves, each of them Across or Down. As soon as one player moves Down, the game ends. It is a common interest game; each player gets the payout at the terminal node.

Since there is only one path to each decision node, a strategy merely has to consist of a set of plans for what to play if that node is reached. Alice's strategy will consist of two capitalised moves, and Bob's strategy will consist of one lower-case move.

If we apply backwards induction, we get that the unique solution of the game is \tol{A_1A_2, a}. Alice would play $A_2$ if it gets that far. Given that, Bob would be better off playing $a$ than $d$ if he gets to play. And given that, Alice is better off playing $A_1$ than $D_1$.

But there are many other Nash equilibria of the game. One of them is \tol{D_1A_2, d}. Given that Player I is playing $D_1$, Player II can play anything without changing her payout; it will be 2 come what may. Given that Player II is playing $d$, Player I is best off playing $D_1$ and taking 2, rather than just getting the 1 that comes from leaving Player II to make a play.

Could this \eqm\ be one that rational players, who know each other to be rational, reach? Stalnaker argues that it could. Assume each player knows that the other player is rational, and is playing that strategy. Given what Player I knows, her strategy choice is clearly rational. She takes 2 rather than the 1 that she would get by playing $A_1$, and she is disposed to take 3 rather than 0 if she gets to the final decision node. So her actual choice is rational, and her disposition is to make another rational choice if we reach the end of the game.

Things are a little trickier for Player II. You might think it is impossible for rational Player II, who knows Player I to be rational, to move $d$. After all, if Player I is rational, then she'll play $A_2$, not $D_2$. And if she plays $A_2$, it is better to play $a$ than $d$. So it looks hard to justify Player II's move. But looks can be deceiving. In fact there isn't anything wrong with Player II's move, as long as he has the right beliefs to justify it. It's very important to distinguish the following two conditionals.

\begin{itemize*}
\item If Player I has the choice, she chooses $A_2$ over $D_2$.
\item If Player I were to have the choice, she would choose $A_2$ over $D_2$.
\end{itemize*}

\noindent Player II knows that the first, indicative, conditional is true. And indeed it is true. But he doesn't know that the second, subjunctive, conditional is true. After all, if Player I were to have the choice between $A_2$ and $D_2$, she would have, \textit{irrationally}, chosen $A_1$ over $D_1$. And if she had chosen irrationally once, it's possible that she would choose irrationally again.

Here's an analogy that may help explain what's going on. The following set of beliefs is consistent.
\begin{itemize*}
\item Any perfectly rational being gets all of the questions on their algebra exam right.
\item Alice is perfectly rational.
\item If Alice had got the second-hardest question on the algebra exam wrong, she would have got the hardest question on the algebra exam wrong as well.
\end{itemize*}
\noindent Player II's beliefs are like that. He believes Player I is perfectly rational. He also believes that if Player I were to make an irrational move, she would continue to make irrational moves. That's consistent with belief in perfect rationality, and nothing about the game setup rules out such a belief. He also believes that playing $A_1$ would be irrational. That's correct, given what Player I knows about Player II. Given all those beliefs, playing $d$, if he had the chance, would be rational.

Stalnaker argues that many game theorists have tacitly confused indicative and subjunctive conditionals in reasoning about games like \citegame{Stalnaker}. Let's look at some other games where similar reasoning takes place.

\section*{Money Burning Game}
Elchanen Ben-Porath and Eddie Dekel suggested this variant to the Battle of the Sexes game. The variation is in two steps. First, we change the payouts for the basic game to the following. (Note that I'm using a lowercase letter for one of $R$'s options here; this is to distinguish the $d$ of down from the $D$ of Don't burn.)

\starttab{r c c}
 & $l$ & $r$ \\
$u$ & 4, 1 & 0, 0 \\
$d$ & 0, 0 & 1, 4 \\
\fintab Then we give Player I the option of publicly burning 2 utils before playing this game. We will use $D$ for Don't burn, and $B$ for burn. So actually each player has four choices. Player I has to choose both $D$ or $B$, then $u$ or $d$. Player 2 has to choose whether to play $l$ or $r$ in each of the two possibilities: first, when $D$ is played, second, when $B$ is played. We'll write $lr$ for the strategy of playing $l$ if $D$, and $r$ if $B$, and so on for the other strategies.

\starttab{r c c c c}
\gamelab{Burning} & $ll$ & $lr$ & $rl$ & $rr$ \\
$Du$ & 4, 1 & 4, 1 & 0, 0 & 0, 0\\
$Dd$ & 0, 0 & 0, 0 & 1, 4 & 1, 4\\
$Bu$ & 2, 1 & -2, 0 & 2, 1 & -2, 0\\
$Bd$ & -2, 0 & -1, 4 & -2, 0 & -1, 4
\fintab Now we can analyse this game a couple of ways. First, we can go through eliminating weakly dominated strategies. Note that $Du$ strictly dominated $Bd$, so we can eliminate it. If $Bd$ is out, then $ll$ weakly dominates $lr$, and $rl$ weakly dominates $rr$. So we can eliminate $lr$ and $rr$. Now $Bu$ weakly dominates $Dd$, relative to the remaining options, so we can eliminate it. Given that just $Du$ and $Bu$ remain, $ll$ weakly dominates all options for Player II, so it is all that is left. And given that $ll$ is all that remains, $Du$ strongly dominates $Bu$. So the iterative deletion of weakly dominated strategies leaves us with \tol{Du, ll} as the unique solution of the game.

Alternatively, we can think the players reason as follows. (The following reasoning is sometimes called `forward induction' reasoning.) Playing $Bd$ is obviously irrational for Player I, since its maximum return is -1, and playing $Du$ or $Dd$ guarantees getting at least 0. So any rational player who plays $B$ must be playing $Bu$. That is, if Player I is rational and plays $B$, she will play $Bu$. Moreover, this is common knowledge among the players. So if Player I plays $B$, she will recognise that Player II will know that she is playing $Bu$. And if Player II knows Player I is playing $u$ after burning, she will play $l$, since that returns her 1 rather than 0. So Player I knows that playing $B$ leads to the 2,1 state; i.e., it returns her 2. But now it is irrational to play $Dd$, since that gets at most 1, and playing $B$ leads to a return of 2. So Player I will play $Bu$ or $Du$. And since the reasoning that leads to this is common knowledge, Player II must play $ll$, since playing $l$ is her best response to a play of $u$, and she knows Player I will play $u$. But if Player II is going to play $ll$, Player I doesn't need to burn; she can just play $Du$.

There is a crucial conditional in the middle of that argument; let's isolate it.

\begin{itemize*}
\item If Player I is rational and plays $B$, she will play $Bu$.
\end{itemize*}

\noindent But that's not what is needed to make the argument work. What Player I needs, and in fact needs to be common knowledge, is the following subjunctive.

\begin{itemize*}
\item If Player I is rational then, if she were to play $B$, she would play $Bu$.
\end{itemize*}

\noindent But in fact there's no reason to believe that. After all, if the forward induction argument is right, then it is \textit{irrational} to burn the money. So if Player I were to play $B$, i.e., burn the money, then she would be doing something irrational. And the fact that Player I is actually rational is consistent with the counterfactual that if she were to do one irrational thing, it would be because she is following an irrational strategy. Note that on the assumption that $Du$ is optimal, then if Player I finds herself having played $B$, it isn't clear that playing $d$ is irrational; it isn't like it is dominated by $u$.

So it isn't true that if Player I were to burn the utils, Player II would know that she is playing $Bu$, and react accordingly by playing $l$. And if that's not true, then there's no reason to think that $Bu$ is better than $Dd$. And if there's no reason to think that, there's no reason to be confident Player 2 will play $ll$.

Stalnaker goes further and provides a positive model where the players start with a common belief in rationality, and in what each other will do, but in which the players play $Bu$ and $rl$. I'll leave it as an exercise to work out what counterfactuals the agents have to believe to make this rational, but suffice to say that it could be a perfectly rational solution.

There is another, relatively simple, \eqm\ to the game. Player I will play $Dd$, and Player II will play $rr$. Player II is certain that Player I will play $d$, and will keep that belief even if Player I irrationally burns 2 utils to start with. Given that certainty, it is rational for Player I to play $Dd$, so Player II's belief is consistent with believing Player I to be rational. And since Player I is playing $Dd$, playing $rr$ is perfectly rational for Player II; indeed it results in her best possible outcome. Moreover, given that Player II is playing $rr$, it makes sense for Player I to play $d$ even if they were, irrationally, to burn the utils. So even though playing $Bd$ would be irrational, since it is strictly dominated, if they were to (irrationally) play $D$, they \textit{should} follow that by playing $d$. There's an important point about the scope of the claim that playing $Bd$ is irrational. It \textit{doesn't} imply that if the player were to irrationally play $B$, it would be irrational to follow with a play of $d$. If Player I was convinced that Player II was playing $rr$, then it would be rational to follow $B$ with $d$.

\section*{Iterated Prisoners' Dilemma}
Let's start with a slightly relabeled version of \citegame{PD-Selfish}, where we use $C$ and $D$ for cooperate and defect.

\starttab{r c c}
\textbf{\citegame{PD-Selfish}} & $c$ & $d$ \\
$C$ & 3, 3 & 0, 5 \\
$D$ & 5, 0 & 1, 1 \\
\fintab We'll call the players I and II, and use uppercase letters for Player I's strategies, and lower case letters for Player II's strategies. We'll assume that each player knows that the other players are perfectly rational in Stalnaker's sense.

There are a couple of arguments that the players must end up at the \eqm\ where every player defects on every move. One of these is an argument by backwards induction. 

At the last move of the game, defecting dominates cooperating. Both players know this. At the second-last move, you might have thought antecedently that there was some benefit to cooperating. After all, it might induce cooperation in the other player. But the only benefit one could get from cooperating was cooperation at the next (i.e., last) move. And no rational player will cooperate on the last move. So, if you're playing with a rational player, there's no benefit to cooperating on the second-last move. But if that's right, and everyone knows it, then there is no benefit to cooperating on the third-last move. The only benefit would be if that would induce cooperation, and it couldn't, since we just proved that any rational player would defect on the second-last move. And so on for any finite length game that you like.

Alternatively, we could look at the game from a strategic perspective. As we've seen in games like \citegame{Burning}, sometimes there are Nash equilibria that don't appear in backwards induction reasoning. But this isn't (\textit{I think}) the case in finitely iterated Prisoners' Dilemma. The only Nash \eqm\ is that both players defect in every circumstance.

This is a little tricky to check since the strategic form of iterated Prisoners' Dilemma gets very complex very quickly. Let's just consider the three round version of the game. Already each player has 128 strategies to choose from. The choice of a strategy involves making 7 distinct choices:

\begin{enumerate*}
\item What to do in the first round.
\item What to do in the second round if the other player cooperates in the first round.
\item What to do in the second round if the other player defects in the first round.
\item What to do in the third round if the other player cooperates in each of the first two rounds.
\item What to do in the third round if the other player cooperates in the first round then defects in the second round.
\item What to do in the third round if the other player defects in the first round then cooperates in the second round.
\item What to do in the third round is the other player defects in each fo the first two rounds.
\end{enumerate*}

\noindent Since these 7 choices are distinct, and the player has 2 choices at each point, there are $2^7 = 128$ possible strategies. So the strategic form of the table involves $128 \times 128 = 16384$ cells. Needless to say, we \textit{won't} be putting that table here. (Though it isn't too hard to get a computer to draw the table for you. The four round game, where each player has $2^{15}$ choices, and there are over one billion cells in the decision table, requires more computing power!)

We'll write a strategy as \tol{x_1x_2x_3x_4x_5x_6x_7}, where $x_i$ is 0 if the player's answer to the $i$'th question above is to cooperate, and 1 if it is to defect. So \tol{0000000} is the strategy of always cooperating, \tol{1111111} is the strategy of always defecting, \tol{0010101} is `tit-for-tat', the strategy of cooperating on the first move, then copying the other player's previous move, and so on.

Let \tol{x_1x_2x_3x_4x_5x_6x_7} be any strategy that doesn't always involve defection on the final round, i.e., a strategy where $x_4 + x_5 + x_6 + x_7 < 4$. It is easy enough to verify that such a strategy is weakly dominated by \tol{x_1x_2x_31111}. In some plays of the game, the defection on the final round leads to getting a better outcome. In other plays of the game, \tol{x_1x_2x_3x_4x_5x_6x_7} and \tol{x_1x_2x_31111} have the same play. For instance, \tol{0001000} will do just as well as \tol{0001111} if the opponent plays any strategy of the form \tol{000x_4x_5x_6x_7}, but it can never do better than \tol{0001111}. So if each player is perfectly rational, we can assume their strategy ends with $1111$.

That cuts each player's choices down to 8. Let's do that table. We'll use $C$ and $c$ rather than 0, and $D$ and $d$ rather than 1, so it is easier to distinguish the two players' strategies. When we label the table, we'll leave off the trailing $D$s and $d$'s, since we assume players are defecting on the last round. We'll also leave off the 3 units the players each get in the last round. (In other words, this will look a lot like the table for a \textit{two} round iterated Prisoners' Dilemma, but it is crucial that it is actually a three-round game.)

%\starttab{r c c c c c c c c}
%\gamelab{PDTable} & $ccc$ & $ccd$ & $cdc$ & $cdd$ & $dcc$ & $dcd$ & $ddc$ & $ddd$ \\
%$CCC$ & 14, 14 & 14, 14 & 7, 16 & 7, 16 & 7, 16 & 7, 16 & 0, 18 & 0, 18 \\
%$CCD$ & 14, 14 & 14, 14 & 7, 16 & 7, 16 & 9 & 9 & 3 & 3 \\
%$CDC$ & 16, 7 & 16, 7 & 10, 10 & 10, 10 & 7 & 7 & 0 & 0 \\
%$CDD$ & 16, 7 & 16, 7 & 10, 10 & 10, 10 & 9 & 9 & 3 & 3 \\
%$DCC$ & 16, 7 & 9, 9 & 16, 7 & 9& 10 & 3 & 10 & 3 \\
%$DCD$ & 16, 7 & 9 ,9 & 16, 7 & 9& 12 & 6 & 12 & 6 \\
%$DDC$ & 18, 0 & 12, 3 & 18, 0 & 12 & 10 & 3 & 10 & 3 \\
%$DDD$ & 18, 0 & 12, 3 & 18, 0 & 12& 10 & 6 & 12 & 6 \\
%\fintab
\starttab{r c c c c c c c c}
\gamelab{PDTable} & $ccc$ & $ccd$ & $cdc$ & $cdd$ & $dcc$ & $dcd$ & $ddc$ & $ddd$ \\
$CCC$ & 6, 6 & 6, 6 & 3, 8  & 3, 8  & 3, 8 & 3, 8 & 0, 10 & 0, 10\\
$CCD$ & 6, 6 & 6, 6 & 3, 8  & 3, 8  & 5, 5 & 5, 5 & 1, 6 & 1, 6\\
$CDC$ & 6, 6 & 8, 3 & 4, 4  & 4, 4  & 3, 8 & 3, 8 & 0, 10 & 0, 10\\
$CDD$ & 6, 6 & 8, 3 & 4, 4  & 4, 4  & 5, 5 & 5, 5 & 1, 6 & 1, 6\\
$DCC$ & 6, 6 & 5, 5 & 8, 3  & 5, 5  & 4, 4 & 1, 6 & 4, 4 & 1, 6\\
$DCD$ & 6, 6 & 5, 5 & 8, 3  & 5, 5  & 6, 1 & 2, 2 & 6, 1 & 2, 2\\
$DDC$ & 6, 6 & 6, 1 & 10, 0 & 6, 1 & 4, 4 & 1, 6 & 4, 4 & 1, 6\\
$DDD$ & 6, 6 & 6, 1 & 10, 0 & 6, 1 & 6, 1 & 2, 2 & 6, 1 & 2, 2\\
\fintab In this game $CCC$ is \textit{strongly} dominated by $CDD$, and $DCC$ is strongly dominated by $DDD$. Similarly $ccc$ and $dcc$ are strongly dominated. So let's delete them.

\starttab{r c c c c c c c c}
\textbf{\citegame{PDTable}}$^\prime$ & $ccd$ & $cdc$ & $cdd$ & $dcd$ & $ddc$ & $ddd$ \\
$CCD$ & 6, 6 & 3, 8  & 3, 8  & 5, 5 & 1, 6 & 1, 6\\
$CDC$ & 8, 3 & 4, 4  & 4, 4  & 3, 8 & 0, 10 & 0, 10\\
$CDD$ & 8, 3 & 4, 4  & 4, 4  & 5, 5 & 1, 6 & 1, 6\\
$DCD$ & 5, 5 & 8, 3  & 5, 5  & 2, 2 & 6, 1 & 2, 2\\
$DDC$ & 6, 1 & 10, 0 & 6, 1 & 1, 6 & 4, 4 & 1, 6\\
$DDD$ & 6, 1 & 10, 0 & 6, 1 & 2, 2 & 6, 1 & 2, 2\\
\fintab Note that each of these is a best response. In particular,
\begin{itemize*}
\item $CCD$ is a best response to $dcd$.
\item $CDC$ is a best response to $ccd$.
\item $CDD$ is a best response to $ccd$ and $dcd$.
\item $DCD$ is a best response to $ddc$.
\item $DDC$ is a best response to $cdc$ and $cdd$.
\item $DDD$ is a best response to $cdc$, $cdd$, $ddc$ and $ddd$.
\end{itemize*}
Now the only Nash \eqm\ of that table is the bottom-right corner. But there are plenty of strategies that, in a strategic version of the game, would be consistent with common belief in perfect rationality. The players could, for instance, play $CDC$ and $cdc$, thinking that the other players are playing $ccd$ and $CCD$ respectively. Those beliefs would be false, but they wouldn't be signs that the other players are irrational, or that they are thinking the other players are irrational.

But you might suspect that the strategic form of the game and the extensive form are crucially different. The very fact that there are Nash equilibria that are not subgame perfect equilibria suggests that there are tighter constraints on what can be played in an extensive game consistent with rationality and belief in rationality. Stalnaker argues, however, that this isn't right. In particular, any strategy that is (given the right beliefs) perfectly rational in the strategic form of the game is also (given the right beliefs and belief updating dispositions) perfectly rational in the extensive form. We'll illustrate this by working more slowly through the argument that the game play \tol{CDC, cdc} is consistent with common belief in perfect rationality.

The first thing you might worry about is that it isn't clear that $CDC$ is perfectly rational, since it is looks to be weakly dominated by $CDD$, and perfect rationality is inconsistent with playing weakly dominated strategies. But in fact $CDC$ isnt weakly dominated by $CDD$. It's true that on the six columns represented here, $CDC$ never does better than $CDD$. But remember that each of these strategies is short for a \textit{three}-round strategy; they are really short for $CDCDDDD$ and $CDDDDDD$. And $CDCDDDD$ is not weakly dominated by $CDDDDDD$; it does better, for example, against $dcccddd$. That is the strategy is defecting the first round, cooperating the second, then defecting on the third round unless the other player has cooperated on each of the first two rounds. Since $CDC$ does cooperate each of the first two rounds, it gets the advantage of defecting against a cooperator on the final round, and ends up with 8 points, whereas $CDD$ merely ends up with 6.

But why would we think Player II might play $dcccddd$? After all, it is itself a weakly dominated strategy, and perfectly rational beings (like Player II) don't play weakly dominated strategies. But we're not actually thinking Player II \textit{will} play that. Remember, Player I's assumption is that Player II will play $ccddddd$, which is not a weakly dominated strategy. (Indeed, it is tit-for-tat-minus-one, which is a well-known strategy.) What Player I also thinks is that if she's wrong about what Player II will play, then it is possible that Player II is not actually perfectly rational. That's consistent with believing Player II is actually perfectly rational. The assumption of common belief in perfect rationality is not sufficient for the assumption that one should believe that the other player is perfectly rational \textit{no matter what surprises happen}. Indeed, that assumption is barely coherent; some assumptions are inconsistent with perfect rationality.

One might be tempted by a weaker assumption. Perhaps a player should hold on to the belief that the other player is perfectly rational unless they get evidence that is \textit{inconsistent} with that belief. But it isn't clear what could motivate that. In general, when we are surprised, we have to give up something that isn't required by the surprise. If we antecedently believe $p \wedge q$, and learn $\neg (p \wedge q)$, then what we've learned is inconsistent with neither $p$ nor $q$, but we must give up one of those beliefs. Similarly here, it seems we must be prepared to give up a belief in the perfect rationality of the other player in some circumstances when that is not entailed by our surprise. (Note that even if you don't buy the argument of this paragraph, there still isn't a reason to think that perfectly rational players can't play $CDD$. But analysing that possibility is beyond the scope of these notes.)

What happens in the extensive form version of the game? Well, each player cooperates, thinking this will induce cooperation in the other, then each player is surprised by a defection at round two, then at the last round they both defect because it is a one-shot Prisoners' Dilemma. Nothing seems irrational there. We might wonder why neither defected at round one. Well, if they believed that the other player was playing tit-for-tat-minus-one, then it is better to cooperate at round one (and collect 8 points over the first two rounds) than to defect (and collect at most 6). And playing tit-for-tat-minus-one is rational if the other person is going to defect on the first and third rounds, and play tit-for-tat on round two. So as long as Player I thinks that Player II thinks that Player I thinks that she is going to defect on the first and third rounds, and play tit-for-tat on round two, then her cooperation at round one is rational, and consistent with believing that the other player is rational.

But note that we had to attribute an odd belief to Player II. We had to assume that Player II is \textit{wrong} about what Player I will play. It turns out, at least for Prisoners' Dilemma, that this is crucial. If both players are certain that both players are perfectly rational, and have no false beliefs, then the only strategy that can be rationalised is permanent defection. The proof (which I'm leaving out) is in Stalnaker's ``Knowledge, Belief and Counterfactual Reasoning in Games''.

This \textit{isn't} because the no false beliefs principle suffices for backwards induction reasoning in general. In \citegame{Stalnaker}, we can have a model of the game where both players are perfectly rational, and have correct beliefs about what the other player will do, and both those things are common belief, and yet the backwards induction solution is not played. In that game $A$ believes, truly, that $B$ will play $d$, and $B$ believes, truly, that $A$ will play $A_1D_2$. And each of these moves is optimal given (true!) beliefs about the other player's strategy.

But Prisoners' Dilemma is special. It isn't that in a game played between players who know each other to have no false beliefs and be perfectly rational that we must end up at the bottom-right corner of the strategic table. But we must end up in a game where every player defects every time. It could be that Player I thinks Player II is playing either $dcd$ or $ddd$, with $ddd$ being much more, and on that basis  she decides to play $dcd$. And Player II could have the converse beliefs. So we'll end up with the play being \tol{DCD, dcd}. But of course that means each player will defect on the first two rounds, and then again on the last round since they are perfectly rational. In such a case the requirement that each player have no false beliefs won't even be enough to get us to Nash \eqm\, since \tol{DCD, dcd} is not a Nash \eqm. But it is enough to get permanent defection.


